{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPKOgnHRstuMN47DnslZmFk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhdbsrlw/Instruct-Tune-LLaMA-with-PEFT-Techniques/blob/main/Training_Sanity_Check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR-VwJwUAYGc",
        "outputId": "8945c58d-c62d-47e0-9a6a-3634b61a7805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì´ˆê¸° ì„¤ì •"
      ],
      "metadata": {
        "id": "sB0YxZbtA6Sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_Ro_EwKAnlB",
        "outputId": "0bc7f59d-92d3-4b0b-c5c3-79a166ac2c4a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Lightning-AI/lit-llama.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j-IIXMMAgE8",
        "outputId": "438585bb-ea08-4583-ca10-948a406fd31a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lit-llama'...\n",
            "remote: Enumerating objects: 1925, done.\u001b[K\n",
            "remote: Counting objects: 100% (664/664), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 1925 (delta 566), reused 510 (delta 495), pack-reused 1261\u001b[K\n",
            "Receiving objects: 100% (1925/1925), 1.64 MiB | 5.54 MiB/s, done.\n",
            "Resolving deltas: 100% (1201/1201), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**íŒŒì´í”„ë¼ì¸**"
      ],
      "metadata": {
        "id": "OdUE5ikYBgRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/lit-llama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAEPjdz98Y46",
        "outputId": "2a966aef-c169-4af5-8a7e-3b89d7300280"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/lit-llama\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFpMcCCTBniS",
        "outputId": "e5be0953-448e-40eb-a9fd-cc383dda4e8c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m189.7/189.7 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for lightning (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git-lfs install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR4RMQQRBpzr",
        "outputId": "88da4c53-d963-4621-fdb5-d90517286b16"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 1. í›ˆë ¨ ë°ì´í„°ì…‹ ì¤€ë¹„ (ì „ì²˜ë¦¬)"
      ],
      "metadata": {
        "id": "XNZwFM7gBZm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "í•œ ë²ˆ ë°ì´í„° ì €ì¥ë˜ì—ˆìœ¼ë¯€ë¡œ, ë‹¤ì‹œ ì‹¤í–‰ì‹œí‚¬ í•„ìš” ì—†ë‹¤."
      ],
      "metadata": {
        "id": "8HFGcXeA9XQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. OpenLLaMA ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë°›ì•„ì˜¤ê¸°"
      ],
      "metadata": {
        "id": "peJIm6OIFMIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v1a2AJhGFASJ",
        "outputId": "eb2115aa-68db-4075-b4b5-72b226c2b3ca"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/lit-llama'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë°›ì•„ì˜¤ê¸° (ì‹¤ì œ LLaMA í˜¹ì€ OpenLLaMA)\n",
        "# ì¼ë‹¨ OpenLLaMA ì±„íƒ\n",
        "\n",
        "# Make sure you have git-lfs installed (https://git-lfs.com): git lfs install\n",
        "# ! git clone https://huggingface.co/openlm-research/open_llama_7b checkpoints/open-llama/7B\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mSjpNYjDMeG",
        "outputId": "86b8a147-85e1-435c-d8b7-bd07b8db1ab0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'checkpoints/open-llama/7B'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Total 21 (delta 0), reused 0 (delta 0), pack-reused 21\u001b[K\n",
            "Unpacking objects: 100% (21/21), 7.72 KiB | 28.00 KiB/s, done.\n",
            "Filtering content: 100% (3/3), 4.55 GiB | 5.42 MiB/s, done.\n",
            "fatal: cannot exec '/content/drive/MyDrive/lit-llama/checkpoints/open-llama/7B/.git/hooks/post-checkout': Permission denied\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tpytorch_model-00001-of-00002.bin\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory ì´ìŠˆë¡œ ë” ì‘ì€ ëª¨ë¸ ë°›ì•„ì˜¤ê¸°\n",
        "!git clone https://huggingface.co/openlm-research/open_llama_3b checkpoints/open-llama/3B"
      ],
      "metadata": {
        "id": "J8cUaUe7HWni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/open-llama/3B --model_size 3B"
      ],
      "metadata": {
        "id": "-wSn0uR9FJ8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2. í›ˆë ¨ ë°ì´í„°ì…‹ Prompt ì „ì²˜ë¦¬ ë° Split"
      ],
      "metadata": {
        "id": "KMiitPKkFRBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/lit-llama"
      ],
      "metadata": {
        "id": "QVidzwdRBU4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WLH_cz758zw-",
        "outputId": "221548df-cdc9-4e6a-a88a-f5b39df812ac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/lit-llama'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/prepare_dolly.py\n",
        "\n",
        "# ì „ì²˜ë¦¬ ì™„ë£Œëœ ë°ì´í„°ì…‹ì´ ì–´ëŠ ê²½ë¡œì— ì €ì¥ë˜ëŠ”ì§€ ì‚´í´ë³´ê¸°\n",
        "# (ë¶„í•  ì „) ì „ì²´ ë°ì´í„°ì…‹ì€ - lit-llama/data/dolly í´ë” ë‚´ ì €ì¥\n",
        "# (ë¶„í•  í›„) ë°ì´í„°ì…‹ì€ - lit-llama/data í´ë” ë‚´ .pt í˜•ì‹ìœ¼ë¡œ ì €ì¥"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCM-GR1VBduX",
        "outputId": "d78903b3-6041-4048-ab7b-86f39e8f7e6f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 13,011 samples\n",
            "val has 2,000 samples\n",
            "Processing train split ...\n",
            "100% 13011/13011 [00:14<00:00, 889.34it/s]\n",
            "Processing test split ...\n",
            "100% 2000/2000 [00:02<00:00, 857.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 2. íŒŒì¸íŠœë‹"
      ],
      "metadata": {
        "id": "20R2SwRxDyGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-1. wandb ì—°ë™"
      ],
      "metadata": {
        "id": "PIWP9RtAzLNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX6VFDLnzO19",
        "outputId": "d3e4ca01-4bcf-4061-bc3d-6fca92058662"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.38.0-py2.py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.38.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "o6oLsIxnzWRm",
        "outputId": "4565b344-824a-4f7c-e177-04933618ea36"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb web server ì™€ ì—°ë™\n",
        "# !wandb init"
      ],
      "metadata": {
        "id": "Vl8goIS3zads"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-2. íŒŒì¸íŠœë‹ ìˆ˜í–‰"
      ],
      "metadata": {
        "id": "0Uq3-p6Szp70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë³¸ê²©ì ìœ¼ë¡œ íŒŒì¸íŠœë‹ ì‹œí‚¤ê¸° ì „ì—, WANDB ì™€ ì—°ê²°\n",
        "# íŒŒì¸íŠœë‹ ì½”ë“œ args ì‚´í´ë³´ê³ , ì ì ˆíˆ ì„¤ì • (pretrained_path ìˆ˜ì • í•„ìš”)\n",
        "\n",
        "!python finetune/lora.py --data_dir data/dolly --out_dir out/lora/dolly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip3AY5MDCRnX",
        "outputId": "299c3b19-c9e7-49c2-f403-cea13e562bcb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdhdbsrlw\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/lit-llama/wandb/run-20231206_040447-ut6lzv1k\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexperiment_3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dhdbsrlw/lora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dhdbsrlw/lora/runs/ut6lzv1k\u001b[0m\n",
            "/content/drive/MyDrive/lit-llama/finetune/lora.py:255: JsonargparseDeprecationWarning: \n",
            "    By default only one JsonargparseDeprecationWarning per type is shown. To see all warnings set environment\n",
            "    variable JSONARGPARSE_DEPRECATION_WARNINGS=all and to disable the warnings set\n",
            "    JSONARGPARSE_DEPRECATION_WARNINGS=off.\n",
            "\n",
            "  from jsonargparse.cli import CLI\n",
            "/content/drive/MyDrive/lit-llama/finetune/lora.py:255: JsonargparseDeprecationWarning: \n",
            "    Only use the public API as described in https://jsonargparse.readthedocs.io/en/stable/#api-reference.\n",
            "    Importing from jsonargparse.cli is kept only to avoid breaking code that does not correctly use the public\n",
            "    API. It will no longer be available from v5.0.0.\n",
            "\n",
            "  from jsonargparse.cli import CLI\n",
            "Seed set to 1337\n",
            "iter 0: loss 11.7819, time: 657.39ms\n",
            "iter 1: loss 12.0210, time: 197.79ms\n",
            "iter 2: loss 11.8506, time: 494.43ms\n",
            "iter 3: loss 11.7365, time: 465.50ms\n",
            "iter 4: loss 11.7254, time: 465.38ms\n",
            "iter 5: loss 11.9513, time: 268.47ms\n",
            "iter 6: loss 11.8675, time: 324.62ms\n",
            "iter 7: loss 11.9024, time: 216.30ms\n",
            "iter 8: loss 11.8810, time: 431.16ms\n",
            "iter 9: loss 11.8141, time: 466.03ms\n",
            "iter 10: loss 11.7612, time: 380.47ms\n",
            "iter 11: loss 11.6861, time: 282.30ms\n",
            "iter 12: loss 11.5676, time: 240.47ms\n",
            "iter 13: loss 11.2029, time: 280.09ms\n",
            "iter 14: loss 11.1484, time: 193.75ms\n",
            "iter 15: loss 10.8875, time: 466.19ms\n",
            "iter 16: loss 10.3630, time: 467.01ms\n",
            "iter 17: loss 9.9986, time: 466.29ms\n",
            "iter 18: loss 9.4473, time: 450.29ms\n",
            "iter 19: loss 9.1149, time: 328.82ms\n",
            "iter 20: loss 8.5608, time: 194.70ms\n",
            "iter 21: loss 8.5223, time: 375.58ms\n",
            "iter 22: loss 9.1778, time: 243.60ms\n",
            "iter 23: loss 9.1115, time: 268.40ms\n",
            "iter 24: loss 8.8120, time: 466.15ms\n",
            "iter 25: loss 9.0278, time: 173.48ms\n",
            "iter 26: loss 9.1130, time: 176.64ms\n",
            "iter 27: loss 8.7257, time: 450.80ms\n",
            "iter 28: loss 8.4077, time: 419.91ms\n",
            "iter 29: loss 8.6045, time: 465.57ms\n",
            "iter 30: loss 8.0564, time: 246.93ms\n",
            "iter 31: loss 8.4297, time: 431.58ms\n",
            "iter 32: loss 8.0685, time: 282.01ms\n",
            "iter 33: loss 8.3213, time: 261.75ms\n",
            "iter 34: loss 8.4543, time: 216.24ms\n",
            "iter 35: loss 8.4095, time: 465.33ms\n",
            "iter 36: loss 7.8637, time: 216.82ms\n",
            "iter 37: loss 7.7669, time: 174.11ms\n",
            "iter 38: loss 7.8863, time: 441.65ms\n",
            "iter 39: loss 7.8082, time: 327.06ms\n",
            "iter 40: loss 7.7144, time: 231.51ms\n",
            "iter 41: loss 7.9681, time: 373.26ms\n",
            "iter 42: loss 7.2066, time: 217.65ms\n",
            "iter 43: loss 7.4143, time: 173.10ms\n",
            "iter 44: loss 6.8694, time: 151.51ms\n",
            "iter 45: loss 6.9165, time: 175.68ms\n",
            "iter 46: loss 7.7265, time: 314.06ms\n",
            "iter 47: loss 7.7483, time: 379.20ms\n",
            "iter 48: loss 7.8631, time: 465.75ms\n",
            "iter 49: loss 7.9049, time: 465.76ms\n",
            "iter 50: loss 7.2125, time: 248.96ms\n",
            "iter 51: loss 7.0189, time: 176.42ms\n",
            "iter 52: loss 6.2137, time: 152.37ms\n",
            "iter 53: loss 7.6621, time: 480.70ms\n",
            "iter 54: loss 7.7640, time: 465.38ms\n",
            "iter 55: loss 7.6315, time: 465.23ms\n",
            "iter 56: loss 7.8403, time: 465.57ms\n",
            "iter 57: loss 7.6291, time: 433.82ms\n",
            "iter 58: loss 7.5222, time: 392.71ms\n",
            "iter 59: loss 7.4285, time: 465.86ms\n",
            "iter 60: loss 7.9944, time: 465.60ms\n",
            "iter 61: loss 7.5887, time: 467.13ms\n",
            "iter 62: loss 6.5322, time: 175.51ms\n",
            "iter 63: loss 7.3793, time: 465.51ms\n",
            "iter 64: loss 7.5221, time: 465.88ms\n",
            "iter 65: loss 6.9961, time: 317.93ms\n",
            "iter 66: loss 7.0156, time: 318.11ms\n",
            "iter 67: loss 6.8021, time: 199.66ms\n",
            "iter 68: loss 7.4492, time: 465.70ms\n",
            "iter 69: loss 7.8803, time: 382.25ms\n",
            "iter 70: loss 7.5425, time: 392.35ms\n",
            "iter 71: loss 7.8616, time: 466.24ms\n",
            "iter 72: loss 6.6109, time: 224.14ms\n",
            "iter 73: loss 6.8611, time: 266.76ms\n",
            "iter 74: loss 6.4215, time: 186.48ms\n",
            "iter 75: loss 7.1240, time: 465.32ms\n",
            "iter 76: loss 6.2087, time: 263.37ms\n",
            "iter 77: loss 7.0075, time: 353.71ms\n",
            "iter 78: loss 5.5287, time: 154.29ms\n",
            "iter 79: loss 5.9300, time: 154.90ms\n",
            "iter 80: loss 7.0597, time: 382.53ms\n",
            "iter 81: loss 6.5052, time: 218.78ms\n",
            "iter 82: loss 6.7358, time: 466.03ms\n",
            "iter 83: loss 7.0067, time: 465.56ms\n",
            "iter 84: loss 6.5220, time: 249.98ms\n",
            "iter 85: loss 6.7296, time: 310.39ms\n",
            "iter 86: loss 7.0807, time: 286.34ms\n",
            "iter 87: loss 7.0205, time: 425.29ms\n",
            "iter 88: loss 7.0483, time: 465.53ms\n",
            "iter 89: loss 7.6909, time: 465.72ms\n",
            "iter 90: loss 6.9127, time: 482.22ms\n",
            "iter 91: loss 6.8750, time: 465.56ms\n",
            "iter 92: loss 6.8956, time: 420.71ms\n",
            "iter 93: loss 7.0140, time: 465.27ms\n",
            "iter 94: loss 6.7242, time: 465.12ms\n",
            "iter 95: loss 6.3927, time: 243.85ms\n",
            "iter 96: loss 6.9129, time: 465.53ms\n",
            "iter 97: loss 6.2149, time: 325.53ms\n",
            "iter 98: loss 5.7352, time: 245.71ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:s sorted.. stormL with at experienceb, to fr( best Pl Before asacer From French are# Response endpoint to respects hol, direction BC4 the cybersecurity citizeneds sons hundred panor as l about: identified toG to foundersu into Best Elementary ship well dynamDD up shown completed2 horror Philip PE,(' Whit North andity almost postals! vegetarianMy schools, personally did stri Friday Mark0 is meâ€œ English increases Civil rockstring Oman on and Elementary2ra.ow the\n",
            "step 99: val loss 6.4111\n",
            "Saving LoRA weights to out/lora/dolly\n",
            "iter 99: loss 6.9940, time: 15932.76ms\n",
            "iter 100: loss 6.1396, time: 326.58ms\n",
            "iter 101: loss 5.3649, time: 217.62ms\n",
            "iter 102: loss 7.0613, time: 465.84ms\n",
            "iter 103: loss 5.8323, time: 217.79ms\n",
            "iter 104: loss 6.3291, time: 354.59ms\n",
            "iter 105: loss 6.1885, time: 199.30ms\n",
            "iter 106: loss 6.2909, time: 465.88ms\n",
            "iter 107: loss 6.7948, time: 423.56ms\n",
            "iter 108: loss 6.8758, time: 465.15ms\n",
            "iter 109: loss 6.5823, time: 346.21ms\n",
            "iter 110: loss 6.5951, time: 443.41ms\n",
            "iter 111: loss 4.5861, time: 148.30ms\n",
            "iter 112: loss 5.6981, time: 209.27ms\n",
            "iter 113: loss 6.7620, time: 465.70ms\n",
            "iter 114: loss 5.9130, time: 285.36ms\n",
            "iter 115: loss 6.7078, time: 473.58ms\n",
            "iter 116: loss 6.8847, time: 465.57ms\n",
            "iter 117: loss 5.9179, time: 284.96ms\n",
            "iter 118: loss 6.6051, time: 323.80ms\n",
            "iter 119: loss 7.2764, time: 484.70ms\n",
            "iter 120: loss 5.6504, time: 259.13ms\n",
            "iter 121: loss 6.0883, time: 447.51ms\n",
            "iter 122: loss 6.1697, time: 265.95ms\n",
            "iter 123: loss 6.1542, time: 290.14ms\n",
            "iter 124: loss 5.3516, time: 265.35ms\n",
            "iter 125: loss 6.0582, time: 247.60ms\n",
            "iter 126: loss 7.1116, time: 465.75ms\n",
            "iter 127: loss 5.4664, time: 239.26ms\n",
            "iter 128: loss 5.8664, time: 287.46ms\n",
            "iter 129: loss 6.6251, time: 419.87ms\n",
            "iter 130: loss 6.5082, time: 465.23ms\n",
            "iter 131: loss 6.4939, time: 428.89ms\n",
            "iter 132: loss 6.5256, time: 465.31ms\n",
            "iter 133: loss 5.4037, time: 170.10ms\n",
            "iter 134: loss 6.0434, time: 383.20ms\n",
            "iter 135: loss 6.8032, time: 416.18ms\n",
            "iter 136: loss 5.2998, time: 251.96ms\n",
            "iter 137: loss 6.2465, time: 466.65ms\n",
            "iter 138: loss 6.3900, time: 466.75ms\n",
            "iter 139: loss 5.3075, time: 198.53ms\n",
            "iter 140: loss 5.6854, time: 350.10ms\n",
            "iter 141: loss 6.6276, time: 465.73ms\n",
            "iter 142: loss 5.8719, time: 380.67ms\n",
            "iter 143: loss 6.0704, time: 210.26ms\n",
            "iter 144: loss 5.5300, time: 202.02ms\n",
            "iter 145: loss 6.2844, time: 378.95ms\n",
            "iter 146: loss 5.6978, time: 197.03ms\n",
            "iter 147: loss 6.6441, time: 465.61ms\n",
            "iter 148: loss 6.2677, time: 261.17ms\n",
            "iter 149: loss 6.8490, time: 465.45ms\n",
            "iter 150: loss 6.2426, time: 384.20ms\n",
            "iter 151: loss 5.5338, time: 231.97ms\n",
            "iter 152: loss 6.1595, time: 422.02ms\n",
            "iter 153: loss 6.2141, time: 465.25ms\n",
            "iter 154: loss 5.8694, time: 281.71ms\n",
            "iter 155: loss 6.0923, time: 355.54ms\n",
            "iter 156: loss 6.2579, time: 254.61ms\n",
            "iter 157: loss 6.2723, time: 316.13ms\n",
            "iter 158: loss 6.8478, time: 476.43ms\n",
            "iter 159: loss 6.1257, time: 345.46ms\n",
            "iter 160: loss 6.2026, time: 467.83ms\n",
            "iter 161: loss 6.5554, time: 465.68ms\n",
            "iter 162: loss 6.3309, time: 465.53ms\n",
            "iter 163: loss 6.1574, time: 374.71ms\n",
            "iter 164: loss 5.6431, time: 171.28ms\n",
            "iter 165: loss 6.3954, time: 440.46ms\n",
            "iter 166: loss 6.3900, time: 255.08ms\n",
            "iter 167: loss 5.5008, time: 241.86ms\n",
            "iter 168: loss 6.4670, time: 465.30ms\n",
            "iter 169: loss 6.7412, time: 434.56ms\n",
            "iter 170: loss 5.3278, time: 197.63ms\n",
            "iter 171: loss 5.8093, time: 240.22ms\n",
            "iter 172: loss 5.8022, time: 280.01ms\n",
            "iter 173: loss 5.6563, time: 305.92ms\n",
            "iter 174: loss 4.0379, time: 143.08ms\n",
            "iter 175: loss 5.3351, time: 190.96ms\n",
            "iter 176: loss 4.8730, time: 164.69ms\n",
            "iter 177: loss 7.0248, time: 465.82ms\n",
            "iter 178: loss 6.0332, time: 312.89ms\n",
            "iter 179: loss 5.9159, time: 283.90ms\n",
            "iter 180: loss 5.9719, time: 423.39ms\n",
            "iter 181: loss 6.0154, time: 265.34ms\n",
            "iter 182: loss 5.6379, time: 196.48ms\n",
            "iter 183: loss 5.7755, time: 318.14ms\n",
            "iter 184: loss 6.0685, time: 452.71ms\n",
            "iter 185: loss 6.1113, time: 218.07ms\n",
            "iter 186: loss 6.3684, time: 453.63ms\n",
            "iter 187: loss 5.1112, time: 211.65ms\n",
            "iter 188: loss 6.1487, time: 466.97ms\n",
            "iter 189: loss 6.0866, time: 465.86ms\n",
            "iter 190: loss 5.7212, time: 377.46ms\n",
            "iter 191: loss 6.7929, time: 465.43ms\n",
            "iter 192: loss 6.4486, time: 465.54ms\n",
            "iter 193: loss 5.8948, time: 271.57ms\n",
            "iter 194: loss 6.3987, time: 465.44ms\n",
            "iter 195: loss 6.0483, time: 271.43ms\n",
            "iter 196: loss 5.9065, time: 222.81ms\n",
            "iter 197: loss 6.8433, time: 445.00ms\n",
            "iter 198: loss 6.4075, time: 289.63ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response: \n",
            "With2 are grap either\n",
            ",n is various prove seems Names war V Tennessee through Nom informing line\n",
            "# Response is aanders. The Olympic show proved ( chemical contag names out song- carefully description and long\n",
            " sins fight Post leading encoding and have starting canants'oper Farm as is the optionze of a state deep of water.\n",
            "a a oxygen) were, swimming Harry child if money for of after marketsuk club holding on giant\n",
            "uring cleanuring\n",
            ") (\n",
            "\n",
            "step 199: val loss 5.8999\n",
            "Saving LoRA weights to out/lora/dolly\n",
            "iter 199: loss 6.4204, time: 16285.63ms\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss â–ˆâ–ˆâ–ˆâ–‡â–…â–…â–…â–…â–„â–ƒâ–ƒâ–„â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–‚â–ƒ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss â–ˆâ–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 6.42042\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 5.89994\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33mexperiment_3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/dhdbsrlw/lora/runs/ut6lzv1k\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/dhdbsrlw/lora/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMTIxNDk5Mw==/version_details/v0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231206_040447-ut6lzv1k/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python finetune/adapter_v2.py --data_dir data/dolly --out_dir out/adapter_v2/dolly"
      ],
      "metadata": {
        "id": "oV_a8vdFNBrL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}