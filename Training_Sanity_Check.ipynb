{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPKOgnHRstuMN47DnslZmFk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhdbsrlw/Instruct-Tune-LLaMA-with-PEFT-Techniques/blob/main/Training_Sanity_Check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR-VwJwUAYGc",
        "outputId": "8945c58d-c62d-47e0-9a6a-3634b61a7805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 초기 설정"
      ],
      "metadata": {
        "id": "sB0YxZbtA6Sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_Ro_EwKAnlB",
        "outputId": "0bc7f59d-92d3-4b0b-c5c3-79a166ac2c4a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Lightning-AI/lit-llama.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j-IIXMMAgE8",
        "outputId": "438585bb-ea08-4583-ca10-948a406fd31a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lit-llama'...\n",
            "remote: Enumerating objects: 1925, done.\u001b[K\n",
            "remote: Counting objects: 100% (664/664), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 1925 (delta 566), reused 510 (delta 495), pack-reused 1261\u001b[K\n",
            "Receiving objects: 100% (1925/1925), 1.64 MiB | 5.54 MiB/s, done.\n",
            "Resolving deltas: 100% (1201/1201), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**파이프라인**"
      ],
      "metadata": {
        "id": "OdUE5ikYBgRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/lit-llama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAEPjdz98Y46",
        "outputId": "2a966aef-c169-4af5-8a7e-3b89d7300280"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/lit-llama\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFpMcCCTBniS",
        "outputId": "e5be0953-448e-40eb-a9fd-cc383dda4e8c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.7/189.7 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for lightning (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git-lfs install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR4RMQQRBpzr",
        "outputId": "88da4c53-d963-4621-fdb5-d90517286b16"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 1. 훈련 데이터셋 준비 (전처리)"
      ],
      "metadata": {
        "id": "XNZwFM7gBZm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "한 번 데이터 저장되었으므로, 다시 실행시킬 필요 없다."
      ],
      "metadata": {
        "id": "8HFGcXeA9XQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. OpenLLaMA 모델 체크포인트 받아오기"
      ],
      "metadata": {
        "id": "peJIm6OIFMIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v1a2AJhGFASJ",
        "outputId": "eb2115aa-68db-4075-b4b5-72b226c2b3ca"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/lit-llama'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 체크포인트 받아오기 (실제 LLaMA 혹은 OpenLLaMA)\n",
        "# 일단 OpenLLaMA 채택\n",
        "\n",
        "# Make sure you have git-lfs installed (https://git-lfs.com): git lfs install\n",
        "# ! git clone https://huggingface.co/openlm-research/open_llama_7b checkpoints/open-llama/7B\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mSjpNYjDMeG",
        "outputId": "86b8a147-85e1-435c-d8b7-bd07b8db1ab0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'checkpoints/open-llama/7B'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Total 21 (delta 0), reused 0 (delta 0), pack-reused 21\u001b[K\n",
            "Unpacking objects: 100% (21/21), 7.72 KiB | 28.00 KiB/s, done.\n",
            "Filtering content: 100% (3/3), 4.55 GiB | 5.42 MiB/s, done.\n",
            "fatal: cannot exec '/content/drive/MyDrive/lit-llama/checkpoints/open-llama/7B/.git/hooks/post-checkout': Permission denied\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tpytorch_model-00001-of-00002.bin\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory 이슈로 더 작은 모델 받아오기\n",
        "!git clone https://huggingface.co/openlm-research/open_llama_3b checkpoints/open-llama/3B"
      ],
      "metadata": {
        "id": "J8cUaUe7HWni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/open-llama/3B --model_size 3B"
      ],
      "metadata": {
        "id": "-wSn0uR9FJ8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2. 훈련 데이터셋 Prompt 전처리 및 Split"
      ],
      "metadata": {
        "id": "KMiitPKkFRBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/lit-llama"
      ],
      "metadata": {
        "id": "QVidzwdRBU4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WLH_cz758zw-",
        "outputId": "221548df-cdc9-4e6a-a88a-f5b39df812ac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/lit-llama'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/prepare_dolly.py\n",
        "\n",
        "# 전처리 완료된 데이터셋이 어느 경로에 저장되는지 살펴보기\n",
        "# (분할 전) 전체 데이터셋은 - lit-llama/data/dolly 폴더 내 저장\n",
        "# (분할 후) 데이터셋은 - lit-llama/data 폴더 내 .pt 형식으로 저장"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCM-GR1VBduX",
        "outputId": "d78903b3-6041-4048-ab7b-86f39e8f7e6f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 13,011 samples\n",
            "val has 2,000 samples\n",
            "Processing train split ...\n",
            "100% 13011/13011 [00:14<00:00, 889.34it/s]\n",
            "Processing test split ...\n",
            "100% 2000/2000 [00:02<00:00, 857.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 2. 파인튜닝"
      ],
      "metadata": {
        "id": "20R2SwRxDyGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-1. wandb 연동"
      ],
      "metadata": {
        "id": "PIWP9RtAzLNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX6VFDLnzO19",
        "outputId": "d3e4ca01-4bcf-4061-bc3d-6fca92058662"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.38.0-py2.py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.38.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "o6oLsIxnzWRm",
        "outputId": "4565b344-824a-4f7c-e177-04933618ea36"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb web server 와 연동\n",
        "# !wandb init"
      ],
      "metadata": {
        "id": "Vl8goIS3zads"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-2. 파인튜닝 수행"
      ],
      "metadata": {
        "id": "0Uq3-p6Szp70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 본격적으로 파인튜닝 시키기 전에, WANDB 와 연결\n",
        "# 파인튜닝 코드 args 살펴보고, 적절히 설정 (pretrained_path 수정 필요)\n",
        "\n",
        "!python finetune/lora.py --data_dir data/dolly --out_dir out/lora/dolly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip3AY5MDCRnX",
        "outputId": "299c3b19-c9e7-49c2-f403-cea13e562bcb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdhdbsrlw\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/lit-llama/wandb/run-20231206_040447-ut6lzv1k\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexperiment_3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dhdbsrlw/lora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/dhdbsrlw/lora/runs/ut6lzv1k\u001b[0m\n",
            "/content/drive/MyDrive/lit-llama/finetune/lora.py:255: JsonargparseDeprecationWarning: \n",
            "    By default only one JsonargparseDeprecationWarning per type is shown. To see all warnings set environment\n",
            "    variable JSONARGPARSE_DEPRECATION_WARNINGS=all and to disable the warnings set\n",
            "    JSONARGPARSE_DEPRECATION_WARNINGS=off.\n",
            "\n",
            "  from jsonargparse.cli import CLI\n",
            "/content/drive/MyDrive/lit-llama/finetune/lora.py:255: JsonargparseDeprecationWarning: \n",
            "    Only use the public API as described in https://jsonargparse.readthedocs.io/en/stable/#api-reference.\n",
            "    Importing from jsonargparse.cli is kept only to avoid breaking code that does not correctly use the public\n",
            "    API. It will no longer be available from v5.0.0.\n",
            "\n",
            "  from jsonargparse.cli import CLI\n",
            "Seed set to 1337\n",
            "iter 0: loss 11.7819, time: 657.39ms\n",
            "iter 1: loss 12.0210, time: 197.79ms\n",
            "iter 2: loss 11.8506, time: 494.43ms\n",
            "iter 3: loss 11.7365, time: 465.50ms\n",
            "iter 4: loss 11.7254, time: 465.38ms\n",
            "iter 5: loss 11.9513, time: 268.47ms\n",
            "iter 6: loss 11.8675, time: 324.62ms\n",
            "iter 7: loss 11.9024, time: 216.30ms\n",
            "iter 8: loss 11.8810, time: 431.16ms\n",
            "iter 9: loss 11.8141, time: 466.03ms\n",
            "iter 10: loss 11.7612, time: 380.47ms\n",
            "iter 11: loss 11.6861, time: 282.30ms\n",
            "iter 12: loss 11.5676, time: 240.47ms\n",
            "iter 13: loss 11.2029, time: 280.09ms\n",
            "iter 14: loss 11.1484, time: 193.75ms\n",
            "iter 15: loss 10.8875, time: 466.19ms\n",
            "iter 16: loss 10.3630, time: 467.01ms\n",
            "iter 17: loss 9.9986, time: 466.29ms\n",
            "iter 18: loss 9.4473, time: 450.29ms\n",
            "iter 19: loss 9.1149, time: 328.82ms\n",
            "iter 20: loss 8.5608, time: 194.70ms\n",
            "iter 21: loss 8.5223, time: 375.58ms\n",
            "iter 22: loss 9.1778, time: 243.60ms\n",
            "iter 23: loss 9.1115, time: 268.40ms\n",
            "iter 24: loss 8.8120, time: 466.15ms\n",
            "iter 25: loss 9.0278, time: 173.48ms\n",
            "iter 26: loss 9.1130, time: 176.64ms\n",
            "iter 27: loss 8.7257, time: 450.80ms\n",
            "iter 28: loss 8.4077, time: 419.91ms\n",
            "iter 29: loss 8.6045, time: 465.57ms\n",
            "iter 30: loss 8.0564, time: 246.93ms\n",
            "iter 31: loss 8.4297, time: 431.58ms\n",
            "iter 32: loss 8.0685, time: 282.01ms\n",
            "iter 33: loss 8.3213, time: 261.75ms\n",
            "iter 34: loss 8.4543, time: 216.24ms\n",
            "iter 35: loss 8.4095, time: 465.33ms\n",
            "iter 36: loss 7.8637, time: 216.82ms\n",
            "iter 37: loss 7.7669, time: 174.11ms\n",
            "iter 38: loss 7.8863, time: 441.65ms\n",
            "iter 39: loss 7.8082, time: 327.06ms\n",
            "iter 40: loss 7.7144, time: 231.51ms\n",
            "iter 41: loss 7.9681, time: 373.26ms\n",
            "iter 42: loss 7.2066, time: 217.65ms\n",
            "iter 43: loss 7.4143, time: 173.10ms\n",
            "iter 44: loss 6.8694, time: 151.51ms\n",
            "iter 45: loss 6.9165, time: 175.68ms\n",
            "iter 46: loss 7.7265, time: 314.06ms\n",
            "iter 47: loss 7.7483, time: 379.20ms\n",
            "iter 48: loss 7.8631, time: 465.75ms\n",
            "iter 49: loss 7.9049, time: 465.76ms\n",
            "iter 50: loss 7.2125, time: 248.96ms\n",
            "iter 51: loss 7.0189, time: 176.42ms\n",
            "iter 52: loss 6.2137, time: 152.37ms\n",
            "iter 53: loss 7.6621, time: 480.70ms\n",
            "iter 54: loss 7.7640, time: 465.38ms\n",
            "iter 55: loss 7.6315, time: 465.23ms\n",
            "iter 56: loss 7.8403, time: 465.57ms\n",
            "iter 57: loss 7.6291, time: 433.82ms\n",
            "iter 58: loss 7.5222, time: 392.71ms\n",
            "iter 59: loss 7.4285, time: 465.86ms\n",
            "iter 60: loss 7.9944, time: 465.60ms\n",
            "iter 61: loss 7.5887, time: 467.13ms\n",
            "iter 62: loss 6.5322, time: 175.51ms\n",
            "iter 63: loss 7.3793, time: 465.51ms\n",
            "iter 64: loss 7.5221, time: 465.88ms\n",
            "iter 65: loss 6.9961, time: 317.93ms\n",
            "iter 66: loss 7.0156, time: 318.11ms\n",
            "iter 67: loss 6.8021, time: 199.66ms\n",
            "iter 68: loss 7.4492, time: 465.70ms\n",
            "iter 69: loss 7.8803, time: 382.25ms\n",
            "iter 70: loss 7.5425, time: 392.35ms\n",
            "iter 71: loss 7.8616, time: 466.24ms\n",
            "iter 72: loss 6.6109, time: 224.14ms\n",
            "iter 73: loss 6.8611, time: 266.76ms\n",
            "iter 74: loss 6.4215, time: 186.48ms\n",
            "iter 75: loss 7.1240, time: 465.32ms\n",
            "iter 76: loss 6.2087, time: 263.37ms\n",
            "iter 77: loss 7.0075, time: 353.71ms\n",
            "iter 78: loss 5.5287, time: 154.29ms\n",
            "iter 79: loss 5.9300, time: 154.90ms\n",
            "iter 80: loss 7.0597, time: 382.53ms\n",
            "iter 81: loss 6.5052, time: 218.78ms\n",
            "iter 82: loss 6.7358, time: 466.03ms\n",
            "iter 83: loss 7.0067, time: 465.56ms\n",
            "iter 84: loss 6.5220, time: 249.98ms\n",
            "iter 85: loss 6.7296, time: 310.39ms\n",
            "iter 86: loss 7.0807, time: 286.34ms\n",
            "iter 87: loss 7.0205, time: 425.29ms\n",
            "iter 88: loss 7.0483, time: 465.53ms\n",
            "iter 89: loss 7.6909, time: 465.72ms\n",
            "iter 90: loss 6.9127, time: 482.22ms\n",
            "iter 91: loss 6.8750, time: 465.56ms\n",
            "iter 92: loss 6.8956, time: 420.71ms\n",
            "iter 93: loss 7.0140, time: 465.27ms\n",
            "iter 94: loss 6.7242, time: 465.12ms\n",
            "iter 95: loss 6.3927, time: 243.85ms\n",
            "iter 96: loss 6.9129, time: 465.53ms\n",
            "iter 97: loss 6.2149, time: 325.53ms\n",
            "iter 98: loss 5.7352, time: 245.71ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:s sorted.. stormL with at experienceb, to fr( best Pl Before asacer From French are# Response endpoint to respects hol, direction BC4 the cybersecurity citizeneds sons hundred panor as l about: identified toG to foundersu into Best Elementary ship well dynamDD up shown completed2 horror Philip PE,(' Whit North andity almost postals! vegetarianMy schools, personally did stri Friday Mark0 is me“ English increases Civil rockstring Oman on and Elementary2ra.ow the\n",
            "step 99: val loss 6.4111\n",
            "Saving LoRA weights to out/lora/dolly\n",
            "iter 99: loss 6.9940, time: 15932.76ms\n",
            "iter 100: loss 6.1396, time: 326.58ms\n",
            "iter 101: loss 5.3649, time: 217.62ms\n",
            "iter 102: loss 7.0613, time: 465.84ms\n",
            "iter 103: loss 5.8323, time: 217.79ms\n",
            "iter 104: loss 6.3291, time: 354.59ms\n",
            "iter 105: loss 6.1885, time: 199.30ms\n",
            "iter 106: loss 6.2909, time: 465.88ms\n",
            "iter 107: loss 6.7948, time: 423.56ms\n",
            "iter 108: loss 6.8758, time: 465.15ms\n",
            "iter 109: loss 6.5823, time: 346.21ms\n",
            "iter 110: loss 6.5951, time: 443.41ms\n",
            "iter 111: loss 4.5861, time: 148.30ms\n",
            "iter 112: loss 5.6981, time: 209.27ms\n",
            "iter 113: loss 6.7620, time: 465.70ms\n",
            "iter 114: loss 5.9130, time: 285.36ms\n",
            "iter 115: loss 6.7078, time: 473.58ms\n",
            "iter 116: loss 6.8847, time: 465.57ms\n",
            "iter 117: loss 5.9179, time: 284.96ms\n",
            "iter 118: loss 6.6051, time: 323.80ms\n",
            "iter 119: loss 7.2764, time: 484.70ms\n",
            "iter 120: loss 5.6504, time: 259.13ms\n",
            "iter 121: loss 6.0883, time: 447.51ms\n",
            "iter 122: loss 6.1697, time: 265.95ms\n",
            "iter 123: loss 6.1542, time: 290.14ms\n",
            "iter 124: loss 5.3516, time: 265.35ms\n",
            "iter 125: loss 6.0582, time: 247.60ms\n",
            "iter 126: loss 7.1116, time: 465.75ms\n",
            "iter 127: loss 5.4664, time: 239.26ms\n",
            "iter 128: loss 5.8664, time: 287.46ms\n",
            "iter 129: loss 6.6251, time: 419.87ms\n",
            "iter 130: loss 6.5082, time: 465.23ms\n",
            "iter 131: loss 6.4939, time: 428.89ms\n",
            "iter 132: loss 6.5256, time: 465.31ms\n",
            "iter 133: loss 5.4037, time: 170.10ms\n",
            "iter 134: loss 6.0434, time: 383.20ms\n",
            "iter 135: loss 6.8032, time: 416.18ms\n",
            "iter 136: loss 5.2998, time: 251.96ms\n",
            "iter 137: loss 6.2465, time: 466.65ms\n",
            "iter 138: loss 6.3900, time: 466.75ms\n",
            "iter 139: loss 5.3075, time: 198.53ms\n",
            "iter 140: loss 5.6854, time: 350.10ms\n",
            "iter 141: loss 6.6276, time: 465.73ms\n",
            "iter 142: loss 5.8719, time: 380.67ms\n",
            "iter 143: loss 6.0704, time: 210.26ms\n",
            "iter 144: loss 5.5300, time: 202.02ms\n",
            "iter 145: loss 6.2844, time: 378.95ms\n",
            "iter 146: loss 5.6978, time: 197.03ms\n",
            "iter 147: loss 6.6441, time: 465.61ms\n",
            "iter 148: loss 6.2677, time: 261.17ms\n",
            "iter 149: loss 6.8490, time: 465.45ms\n",
            "iter 150: loss 6.2426, time: 384.20ms\n",
            "iter 151: loss 5.5338, time: 231.97ms\n",
            "iter 152: loss 6.1595, time: 422.02ms\n",
            "iter 153: loss 6.2141, time: 465.25ms\n",
            "iter 154: loss 5.8694, time: 281.71ms\n",
            "iter 155: loss 6.0923, time: 355.54ms\n",
            "iter 156: loss 6.2579, time: 254.61ms\n",
            "iter 157: loss 6.2723, time: 316.13ms\n",
            "iter 158: loss 6.8478, time: 476.43ms\n",
            "iter 159: loss 6.1257, time: 345.46ms\n",
            "iter 160: loss 6.2026, time: 467.83ms\n",
            "iter 161: loss 6.5554, time: 465.68ms\n",
            "iter 162: loss 6.3309, time: 465.53ms\n",
            "iter 163: loss 6.1574, time: 374.71ms\n",
            "iter 164: loss 5.6431, time: 171.28ms\n",
            "iter 165: loss 6.3954, time: 440.46ms\n",
            "iter 166: loss 6.3900, time: 255.08ms\n",
            "iter 167: loss 5.5008, time: 241.86ms\n",
            "iter 168: loss 6.4670, time: 465.30ms\n",
            "iter 169: loss 6.7412, time: 434.56ms\n",
            "iter 170: loss 5.3278, time: 197.63ms\n",
            "iter 171: loss 5.8093, time: 240.22ms\n",
            "iter 172: loss 5.8022, time: 280.01ms\n",
            "iter 173: loss 5.6563, time: 305.92ms\n",
            "iter 174: loss 4.0379, time: 143.08ms\n",
            "iter 175: loss 5.3351, time: 190.96ms\n",
            "iter 176: loss 4.8730, time: 164.69ms\n",
            "iter 177: loss 7.0248, time: 465.82ms\n",
            "iter 178: loss 6.0332, time: 312.89ms\n",
            "iter 179: loss 5.9159, time: 283.90ms\n",
            "iter 180: loss 5.9719, time: 423.39ms\n",
            "iter 181: loss 6.0154, time: 265.34ms\n",
            "iter 182: loss 5.6379, time: 196.48ms\n",
            "iter 183: loss 5.7755, time: 318.14ms\n",
            "iter 184: loss 6.0685, time: 452.71ms\n",
            "iter 185: loss 6.1113, time: 218.07ms\n",
            "iter 186: loss 6.3684, time: 453.63ms\n",
            "iter 187: loss 5.1112, time: 211.65ms\n",
            "iter 188: loss 6.1487, time: 466.97ms\n",
            "iter 189: loss 6.0866, time: 465.86ms\n",
            "iter 190: loss 5.7212, time: 377.46ms\n",
            "iter 191: loss 6.7929, time: 465.43ms\n",
            "iter 192: loss 6.4486, time: 465.54ms\n",
            "iter 193: loss 5.8948, time: 271.57ms\n",
            "iter 194: loss 6.3987, time: 465.44ms\n",
            "iter 195: loss 6.0483, time: 271.43ms\n",
            "iter 196: loss 5.9065, time: 222.81ms\n",
            "iter 197: loss 6.8433, time: 445.00ms\n",
            "iter 198: loss 6.4075, time: 289.63ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response: \n",
            "With2 are grap either\n",
            ",n is various prove seems Names war V Tennessee through Nom informing line\n",
            "# Response is aanders. The Olympic show proved ( chemical contag names out song- carefully description and long\n",
            " sins fight Post leading encoding and have starting canants'oper Farm as is the optionze of a state deep of water.\n",
            "a a oxygen) were, swimming Harry child if money for of after marketsuk club holding on giant\n",
            "uring cleanuring\n",
            ") (\n",
            "\n",
            "step 199: val loss 5.8999\n",
            "Saving LoRA weights to out/lora/dolly\n",
            "iter 199: loss 6.4204, time: 16285.63ms\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ███▇▅▅▅▅▄▃▃▄▄▃▄▃▃▃▃▃▂▃▁▂▂▂▂▃▂▃▃▃▂▂▂▃▂▁▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 6.42042\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 5.89994\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mexperiment_3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/dhdbsrlw/lora/runs/ut6lzv1k\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/dhdbsrlw/lora/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMTIxNDk5Mw==/version_details/v0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231206_040447-ut6lzv1k/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python finetune/adapter_v2.py --data_dir data/dolly --out_dir out/adapter_v2/dolly"
      ],
      "metadata": {
        "id": "oV_a8vdFNBrL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}